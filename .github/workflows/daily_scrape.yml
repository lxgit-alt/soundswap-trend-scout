name: SoundSwap Daily Blog Scout

on:
  schedule:
    - cron: '0 9 * * *'  # Daily at 9:00 UTC
  workflow_dispatch:  # Allows manual triggering

jobs:
  daily-blog-scout:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4.1.1

      - name: Setup Python
        uses: actions/setup-python@v5.0.0
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-search-results

      - name: Run Full Scout with 4 Topics
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
          SERPAPI_KEY: ${{ secrets.SERPAPI_KEY }}
          DISCORD_TOKEN: ${{ secrets.DISCORD_BOT_TOKEN }}
          DISCORD_CHANNEL_ID: ${{ secrets.DISCORD_CHANNEL_ID }}
        run: |
          python -c "
          import os
          import requests
          import json
          from datetime import datetime
          from serpapi import Client
          
          # Get credentials
          DISCORD_TOKEN = os.getenv('DISCORD_TOKEN')
          CHANNEL_ID = os.getenv('DISCORD_CHANNEL_ID')
          SERPAPI_KEY = os.getenv('SERPAPI_KEY')
          WEBHOOK = os.getenv('DISCORD_WEBHOOK')
          
          if not DISCORD_TOKEN or not CHANNEL_ID:
              print('âŒ Discord credentials missing')
              exit(1)
          
          # 4 Daily Topics
          NICHE_QUERIES = [
              'latest music production gear releases 2024',
              'breaking AI audio tools for artists', 
              'independent music marketing trends 2024',
              'music streaming industry news today'
          ]
          
          def get_trend_score(keyword):
              '''Get Google Trends score.'''
              try:
                  client = Client(api_key=SERPAPI_KEY)
                  results = client.search({
                      'engine': 'google_trends',
                      'q': keyword,
                      'data_type': 'TIMESERIES',
                      'date': 'now 7-d'
                  })
                  timeseries = results.get('interest_over_time', {}).get('timeline_data', [])
                  return int(timeseries[-1].get('values')[0].get('value')) if timeseries else 50
              except Exception as e:
                  print(f'âš ï¸ Trends error for {keyword}: {e}')
                  return 50
          
          def get_serp_data(query):
              '''Get search results for a query.'''
              try:
                  score = get_trend_score(query)
                  client = Client(api_key=SERPAPI_KEY)
                  
                  results = client.search({
                      'engine': 'google',
                      'q': query,
                      'tbs': 'qdr:d',
                      'api_key': SERPAPI_KEY,
                      'num': 3
                  })
                  
                  organic = results.get('organic_results', [])
                  questions = results.get('related_questions', [])[:3] if results.get('related_questions') else []
                  
                  first_result = organic[0] if organic else {}
                  
                  status = 'ğŸ”¥ VIRAL' if score > 75 else 'ğŸ“ˆ TRENDING' if score > 50 else 'ğŸ“Š STEADY'
                  
                  return {
                      'query': query,
                      'score': score,
                      'link': first_result.get('link', 'No link found'),
                      'title': first_result.get('title', ''),
                      'questions': [q.get('question') for q in questions if q.get('question')],
                      'status': status
                  }
              except Exception as e:
                  print(f'âŒ Error for {query}: {e}')
                  return {
                      'query': query,
                      'score': 50,
                      'link': 'No link found',
                      'title': '',
                      'questions': [],
                      'status': 'ğŸ“Š STEADY'
                  }
          
          print('ğŸš€ Starting daily blog scout...')
          
          # Get data for all 4 topics
          daily_topics = []
          for query in NICHE_QUERIES:
              print(f'ğŸ“¡ Fetching: {query}')
              topic_data = get_serp_data(query)
              daily_topics.append(topic_data)
          
          # Build the daily report
          report = f'ğŸ¸ **SOUNDSWAP DAILY BLOG TOPICS** ({datetime.now().strftime("%Y-%m-%d")})\n\n'
          report += '**Choose ONE topic for today\'s semantic SEO blog:**\n\n'
          
          for i, topic in enumerate(daily_topics, 1):
              emoji = ['1ï¸âƒ£', '2ï¸âƒ£', '3ï¸âƒ£', '4ï¸âƒ£'][i-1]
              
              # Preview PAA questions
              paa_preview = topic['questions'][:2] if topic['questions'] else ['What producers need to know']
              
              report += f'{emoji} **{topic["query"].upper()}**\n'
              report += f'   ğŸ“Š Trend: {topic["score"]}/100 {topic["status"]}\n'
              report += f'   ğŸ”— Source: {topic["link"][:50]}...\n'
              if topic['questions']:
                  report += f'   â“ PAA: {paa_preview[0][:40]}...\n\n'
              else:
                  report += f'   â“ PAA: No questions found\n\n'
          
          report += 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n'
          report += '**HOW TO GENERATE YOUR BLOG:**\n'
          report += '1. Type `/blog` in this channel\n'
          report += '2. Choose topic (1-4)\n'  
          report += '3. Choose outline style\n'
          report += '4. Get semantic SEO blog with PAA â†’ H3 headers!\n\n'
          report += 'â±ï¸ *Only 1 blog per day for maximum SEO impact*'
          
          # Send to Discord channel using Bot token
          url = f'https://discord.com/api/v10/channels/{CHANNEL_ID}/messages'
          headers = {
              'Authorization': f'Bot {DISCORD_TOKEN}',
              'Content-Type': 'application/json'
          }
          
          # Split if too long
          max_length = 1900
          chunks = [report[i:i + max_length] for i in range(0, len(report), max_length)]
          
          for i, chunk in enumerate(chunks):
              try:
                  response = requests.post(url, headers=headers, json={'content': chunk})
                  if response.status_code == 200:
                      print(f'âœ… Chunk {i+1}/{len(chunks)} sent')
                  elif response.status_code == 429:
                      print(f'âš ï¸ Rate limited, waiting...')
                      import time
                      time.sleep(2)
                      response = requests.post(url, headers=headers, json={'content': chunk})
                  else:
                      print(f'âš ï¸ Chunk {i+1} failed: {response.status_code} - {response.text}')
              except Exception as e:
                  print(f'âŒ Error sending chunk {i+1}: {e}')
          
          # Also send simple webhook notification if available
          if WEBHOOK:
              webhook_msg = {
                  'content': f'âœ… **Daily Blog Scout Complete**\nğŸ“… {datetime.now().strftime("%Y-%m-%d %H:%M UTC")}\nğŸ“Š Topics analyzed: {len(daily_topics)}\n\nUsers can now use `/blog` to generate their daily semantic SEO blog!'
              }
              try:
                  requests.post(WEBHOOK, json=webhook_msg)
                  print('âœ… Webhook notification sent')
              except Exception as e:
                  print(f'âš ï¸ Webhook error: {e}')
          
          print('ğŸ¯ Daily scout completed successfully!')
          "

      - name: Notify Completion
        if: always()
        run: |
          echo "âœ… GitHub Actions workflow completed"
          echo "ğŸ“… $(date)"